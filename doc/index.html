<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Spark Streaming and HBase Workshop</title>

    <!-- Bootstrap -->
    <link href="./assets/css/bootstrap.min.css" rel="stylesheet">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>
  <body>


    <div class="container">

    <div class="jumbotron">
      <h1>Spark Streaming, HBase and Kafka API Workshop</h1>
      <p>Discover Spark, Spark Streaming, Kafka API and HBase to create a Time Series Application </p>
      <p>In this activity you will build and run a Spark Streaming application. Spark Streaming programs are best run as standalone applications built using Maven or sbt. First we will load and inspect the data using the spark shell , then we will use Spark Streaming from the shell. This will make it easier to understand how to understand the  application code.
      </p>

      <p>Hbase, Spark and Kafka application will be deployed on MapR cluster. (MapR Streams)</p>

    </div>



    <h1>Setup</h1>
	  <section id="installation">
	      <h3>Installation</h3>

      <p>
        For this lab we will use <a href="https://www.mapr.com/products/mapr-sandbox-hadoop" target="_workshop">MapR Hadoop Sandbox 5.2</a> (Virtual Machine) or a MapR Cluster deployed on the cloud. The MapR cluster contains all the necessary components : Apache Kafka/MapR Streams, Apache Spark and Apache HBase/MapR-DB.
      </p>

      <p>
        <ol>
          <li>Copy the <code>mapr-streams-spark-hbase-workshop-master.zip</code> to your laptop<br/><br/></li>

          <li>Copy the <code>mapr-streams-spark-hbase-workshop-master.zip</code> to your Sandbox/Cluster home directory
              <p>
                <pre>scp mapr-streams-spark-hbase-workshop-master.zip [username]@node-ip:/user/[username]/.  </pre>
              </p>
          </li>
          <li>Connect to your Sandbox/Cluster
              <p>
                if you are using Amazon cluster, use the following command:
                  <pre>ssh [username]@node-ip</pre>
              </p>
              <p>
                <br/>
                if you are using MapR sandbox use the following command:
                <pre>ssh user01@localhost -p 2222</pre>
              </p>
              <br/>
          </li>


          <li>Unzip <code>mapr-streams-spark-hbase-workshop-master.zip</code> in your Sandbox/Cluster

<p>

<pre>
  cd /user/$USERID

  unzip mapr-streams-spark-hbase-workshop-master.zip
</pre>

Copy the data into a new folder
<pre>
  mkdir data

  cp ./mapr-streams-spark-hbase-workshop-master/data/* ./data/
</pre>

<p>
The data folder contains a csv file (sensordata.csv … ), that will be used to inject "event" into our aplication.
</p>

<p>
Each line in the CSV looks like:<br>

<pre>
  BBKING,3/14/14,23:46,9.78,0.943,1088,0.24,88,1.86
</pre>

The fields are :
<ul>
<li>resid: Sensor/Resource ID, as string</li>
<li>date: Date of the capture/event as string using MM/DD/YY</li>
<li>time: Time of the event as string using HH:MN</li>
<li>hz: Frequence  as double</li>
<li>disp: double (not used in this application)</li>
<li>flo: double (not used in this application)</li>
<li>sedPPM: double (not used in this application)</li>
<li>psi: pressure as double</li>
<li>chlPPM: double (not used in this application)</li>
</ul>

</p>

<p>

We will use this CSV file to do some spark operation such as:
<ul>
  <li>Read file</li>
  <li>Query data</li>
  <li>Aggregate data</li>
</ul>

Then on the streaming part, the application will push these data into a datastream to capture and analyze the data in real time.<br/><br/>

Note, that the volume of data is pretty small for this example, but it would work the same way with a very large dataset except that the data processing will be distributed on all the nodes of the cluster by Spark and MapR.


</p>
              </p>
              <br/>
          </li>

          <li>Now unzip <code>mapr-streams-spark-hbase-workshop-master.zip</code> on your laptop.<br/>
            <p>
              When you unzip the file mapr-streams-spark-hbase-workshop-master.zip, it will create the following structure. <br/>
              <img src="./assets/images/001.png" />
            </p>
            There is an exercises package with stubbed code for you to finish and there is a solutions package with the complete solution. Open/Import either project into your IDE following the instructions below. Optionally you can just edit the scala files and use maven on the command line, or if you just want to use the prebuilt solution, you can copy the solution jar file from the target directory.

          </li>
        </ol>
      </p>

      <!-- REMOVE IDE SECTION
      <h3>Download/Setup your IDE</h3>

      <p>
        You can use your choice of Netbeans, Intellij, Eclipse, or just a text editor with Apache Maven on the command line. You need to install your IDE of choice on your laptop, or alternatively install maven on the sandbox and use the command line.  If you use Netbeans or Intellij you only need to add the scala plugin, maven is included. If you use Eclipse, depending on the version, you may need to add the maven and scala plugins.
      </p>

      <ul>
        <li>Netbeans:
          <br/><a href="https://netbeans.org/downloads/">https://netbeans.org/downloads/</a>
          <br/>click on tools plugins and add the scala plugin
          <br/><br/>
        </li>
        <li>Eclipse with Scala and maven:
          <br/><a href="http://scala-ide.org/download/sdk.html">http://scala-ide.org/download/sdk.html</a>
          <br/><br/>
      </li>
      <li>IntelliJ:
        <br/><a href="https://www.jetbrains.com/idea/download/">https://www.jetbrains.com/idea/download/</a>
        <br/><br/>
      </li>
      <li>Maven (install on the sandbox, if you just want to edit the files on the sandbox)
          <br/><a href="https://maven.apache.org/download.cgi">https://maven.apache.org/download.cgi</a>
          <br/><br/>
      </li>
      </ul>
       -->

	  </section>







    <hr/>
    <h1>Load and  Inspect data using the Spark Shell </h1>
	  <section id="lab-01">

        <div>
          Log into your sandbox or cluster, with your userid and password mapr:
          <code>ssh user01@ipaddress </code>
        </div>

        <br/><br/>
       <h4>Launch the Spark Interactive Shell</h4>

       <div>
         To launch the Interactive Shell, at the command line, run the following command:
<pre>
 cd

 /opt/mapr/spark/spark-2.0.1/bin/spark-shell --master local
</pre>

       </div>

       <br/><br/>
       <h4>Load Data in Spark RDD</h4>

       <div>
         You can copy paste the code from below, it is also provided as a text file in the lab files <code>shellcommands</code> directory.<br/><br/>
       </div>

       <div>
           First, we will import some packages and instantiate a sqlContext, which is the entry point for working with structured data (rows and columns) in Spark and allows the creation of DataFrame objects.
<pre>
// Import Utility Class
import org.apache.spark.util.StatCounter
</pre>
       </div>

      <div>
        <br/><br/>
        Below we load the data from the csv file into a Resilient Distributed Dataset (RDD). RDDs can have transformations and actions;  the <code>take(1)</code> action returns the first element in the RDD.

<pre>
// load the data into a  new RDD, replace <b>$USERID</b> with the proper path
val textRDD = sc.textFile("/user/<b>$USERID</b>/data/sensordata.csv")

// Return the first element in this RDD
textRDD.take(1)</pre>

<p>
  Count the number of lines in your RDD:
  <pre>textRDD.count()</pre>
</p>


      </div>

     <br/>
     <h4>Transform the RDD into a new RDD</h4>

      <div>
          In the previous command you have loaded the CSV files into an RDD that contains simple string, an array of string.<br/><br/>

          Spark, and Scala allow you to create Class and transform each line of the RDD into this class. The code below:

          <ol>
            <li>Create a new Scala class <code>Sensor</code><br/></li>
            <li>Create a new function <code>parseSensor</code> that parse each line and create an instance of Sensor<br/></li>
            <li>Then use the RDD created above and apply the <code>map()</code> transformation, that will be applied to each element of textRDD to create the RDD of sensor objects.<br/></li>
          </ol>
<pre>
//define the schema using a case class
case class Sensor(
  resid: String,
  date: String,
  time: String,
  hz: Double,
  disp: Double,
  flo: Double,
  sedPPM: Double,
  psi: Double,
  chlPPM: Double
)

// function to parse line of sensor data into Sensor class
def parseSensor(str: String): Sensor = {
  val p = str.split(",")
  Sensor(p(0), p(1), p(2), p(3).toDouble, p(4).toDouble, p(5).toDouble, p(6).toDouble, p(7).toDouble, p(8).toDouble)
}

// create an RDD of sensor objects
val sensorRDD= textRDD.map(parseSensor)

// The RDD first() action returns the first element in the RDD
sensorRDD.take(1)
</pre>

       Take a look to the 2 RDDs you have in your context <code>textRDD</code> and <code>sensorRDD</code>. You should see some differences (class, type, ...)

      </div>


      <br/><br/>
      <h4>Get some statistics from RDD</h4>

      <div>
        Now that you have RDD it is possible to use <code>actions</code> to get some values, or <code>transformations</code> to get new RDD.

<pre>
// Return the number of elements in the RDD
sensorRDD.count()

//  create an alert RDD for when psi is low
val alertRDD = sensorRDD.filter(sensor => sensor.psi < 5.0)

//  print some results
alertRDD.take(3).foreach(println)
</pre>

    </div>

    <div>
      <br/><br/>
      Let's now do some aggregation on the data, and get more statistics
      <ol>
        <li>
          Create a new RDD, as key value that contains the sensor, date and PSI.
        </li>
        <li>
          Group the values by sensor and date and get some stats using <a href="https://spark.apache.org/docs/1.4.1/api/scala/index.html#org.apache.spark.util.StatCounter" target="_workshop">StatCounter</a> class
        </li>
        <li>
          Then print the 3 first values of the new stats RDD
        </li>
      </ol>
<pre>
// transform into an RDD of (key, values) to get daily stats for psi
val keyValueRDD=sensorRDD.map(sensor => ((sensor.resid,sensor.date),sensor.psi))

// print out some data
keyValueRDD.take(3).foreach(kv => println(kv))

// use StatCounter utility to get statistics for sensor psi
val keyStatsRDD = keyValueRDD.groupByKey().mapValues(psi => StatCounter(psi))

// print out some data
keyStatsRDD.take(5).foreach(println)
</pre>

    </div>



<br/><br/>
<h5>Questions:</h5>

<ul>
  <li>
    How many entries have a psi equals to 100?<br/><br/>
    <div><button class="button" onclick="$('#lab01-sol-1').toggle();">Show/Hide Solution</button></div>
    <div id="lab01-sol-1" style="display:none;" >
    <pre>sensorRDD.filter(sensor => sensor.psi == 100.0).count()</pre>
    </div>
    <br/><br/>
  </li>

  <li>
    List the statistics for the PSI per "sensor"<br/><br/>
    <div><button class="button" onclick="$('#lab01-sol-2').toggle();">Show/Hide Solution</button></div>
    <div id="lab01-sol-2" style="display:none;" >
    <pre>sensorRDD.map(sensor => (sensor.resid,sensor.psi)).groupByKey().mapValues(psi => StatCounter(psi)).foreach(println)</pre>
    </div>
    <br/><br/>
  </li>

</ul>




<h1>Analyze the data with DataFrame</h1>
<section id="lab-02">

    <div>
      In this section you will continue to use the opened Spark Shell.<br/><br/>
    </div>

    <div>
      A <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes" target="_workshop">DataFrame</a> is a distributed collection of data organized into named columns. Spark SQL supports automatically converting an RDD containing case classes to a DataFrame with the method <code>toDF()</code>.
    </div>


    <br/><br/>
   <h4>Create DataFramces from existing RDD</h4>

   <div>
     In the following code you will use the <code>toDF()</code> method. The Sensor class will be use to define the DataFrame entries.
<pre>
  // change to a DataFrame
  val sensorDF = sensorRDD.toDF()
</pre>

<br/><br/>
<h5>Question:</h5>

<ul>
  <li>
    How can you create the DataFrame directly from the CSV file? <br/><br/>
    <div><button class="button" onclick="$('#lab02-sol-1').toggle();">Show/Hide Solution</button></div>
    <div id="lab02-sol-1" style="display:none;" >
    <pre>val sensorDF= sc.textFile("/user/<b>$USERID</b>/mapr-streams-spark-hbase-workshop-master/data/sensordata.csv").map(parseSensor).toDF()</pre>
    </div>
    <br/><br/>
  </li>
</ul>
   </div>


   <h4>Explore the data set with queries</h4>

   <div>
     DataFrames API is part of the Spark SQL component. This allows you to execute complex queries on your dataset. In the following section you will use various queries to explore the sensor dataset.
     <br/><br/>
   </div>

<pre>

import org.apache.spark.sql.SparkSession

// Create a spark SQL session; ideallu replace $USERID by your user
val spark = SparkSession.builder().appName("Workshop $USERID").getOrCreate()

// For implicit conversions like converting RDDs to DataFrames
import spark.implicits._

// group by the sensorid, date get average psi
sensorDF.groupBy("resid", "date").agg(avg(sensorDF("psi"))).take(5).foreach(println)

// Displays the content of the DataFrame to stdout
sensorDF.show()

// Display dataframe schema
sensorStatDF.take(5).foreach(println)

// register as a temp table then you can query
sensorDF.createOrReplaceTempView("sensor")

// get the max , min, avg  for each column
val sensorStatDF = spark.sql("SELECT resid, date,MAX(hz) as maxhz, min(hz) as minhz, avg(hz) as avghz, MAX(disp) as maxdisp, min(disp) as mindisp, avg(disp) as avgdisp, MAX(flo) as maxflo, min(flo) as minflo, avg(flo) as avgflo,MAX(sedPPM) as maxsedPPM, min(sedPPM) as minsedPPM, avg(sedPPM) as avgsedPPM, MAX(psi) as maxpsi, min(psi) as minpsi, avg(psi) as avgpsi,MAX(chlPPM) as maxchlPPM, min(chlPPM) as minchlPPM, avg(chlPPM) as avgchlPPM FROM sensor GROUP BY resid,date")

// print out the results
sensorStatDF.take(5).foreach(println)

</pre>
<div>
  <br/><br/>
  The following code extract the first PSI value of the <code>sensorDF</code>
  <pre>sensorDF.select("psi").take(1)</pre>
<div>


  <br/><br/>
  <h5>Questions:</h5>

  <ul>

    <li>
      How can you show the first 100 "psi" values? <br/><br/>
      <div><button class="button" onclick="$('#lab02-sol-21').toggle();">Show/Hide Solution</button></div>
      <div id="lab02-sol-21" style="display:none;" >
      <pre>sensorDF.select("psi").show(100)</pre>
      </div>
      <br/><br/>
    </li>


    <li>
      How can I get the list of all sensor? (field <code>resid</code> - tips: apply distinct on select statement) <br/><br/>
      <div><button class="button" onclick="$('#lab02-sol-2').toggle();">Show/Hide Solution</button></div>
      <div id="lab02-sol-2" style="display:none;" >
      <pre>sensorDF.select("resid").distinct.foreach(r => println(r) )</pre>
      or
      <pre>spark.sql("SELECT DISTINCT(resid) FROM sensor").foreach(r => println(r) )</pre>
      </div>
      <br/><br/>
    </li>
  </ul>


  <br/><br/>
  <h4>Summary</h4>
  <div>
    In this section you have learn how to use the Spark Shell and API to read a CSV file, and create RDDs from String or Classes, and finally use the DataFrame and SparkSQL API to analyze the data.

    <br/><br/>
    In the next section you will use the same data source but use the Streaming API.

  </div>


</section>


<h1>Use Spark Streaming with the Spark Shell</h1>
<section id="lab-03">
      <div>
        <p>
          For this example, the streaming API will use the <b>File Streams</b>, this means that the Spark application will capture automatically any new file as datasource.
        </p>


        <p>
          Spark Streaming supports data sources such as Directories, HDFS directories, TCP sockets, Kafka, Flume, Twitter, etc. Data Streams can be processed with Spark’s core APIS, DataFrames SQL, or machine learning APIs, and can be persisted to a filesystem, HDFS, databases, or any data source offering a Hadoop OutputFormat.
          <br/>
          <img src="./assets/images/009.png"/>
        </p>

        <p>
          Depending of your application you will chose the source. In this example we can imagine that the various sensor are pushing new files in a directory, and each time the application will capture it automatically.
        </p>


      </div>

      <div>
        First create a directory that the streaming application will read from at the linux command line type:
        <pre>mkdir -p /user/$USER/stream</pre>
        <br/><br/>
      </div>

      <h4>Launch the Spark Interactive Shell</h4>

      <div>
        To launch the Interactive Shell for streaming, at the command line, run the following command, we need 2 threads for streaming:
        <pre>/opt/mapr/spark/spark-2.0.1/bin/spark-shell --master local[2]</pre>
        <br/><br/>
      </div>

      <div>
        You can copy paste the code from below, it is also provided as a text file in the lab files shell commands directory.
        <br/><br/>
      </div>

      <div>
        First, we will import some packages related to SparkStreaming library.
      </div>
<pre>
import org.apache.spark.SparkConf
import org.apache.spark.streaming.{Seconds, StreamingContext}
import StreamingContext._
</pre>

<br/>
<br/>
  <ol>
    <li>
      We create the class Sensor that will be used later on. (same as before)
    </li>
    <li>We create a <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.StreamingContext" target="+workshop">StreamingContext</a>, the main entry point for streaming functionality, with <a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#setting-the-right-batch-interval" target="_workshop">a 2 seconds batch interval</a>.
    </li>
    <li>
      Next, we use the StreamingContext textFileStream(directory) method to create an input stream
    </li>
  </ol>

<pre>
case class Sensor(
  resid: String,
  date: String,
  time: String,
  hz: Double,
  disp: Double,
  flo: Double,
  sedPPM: Double,
  psi: Double,
  chlPPM: Double
  ) extends Serializable

val ssc = new StreamingContext(sc, Seconds(2))

val linesDStream = ssc.textFileStream("/user/<b>$USERID</b>/stream/")

linesDStream.print()
</pre>


<div>
  The linesDStream represents the stream of data, each record is a line of text. Internally a DStream is a sequence of RDDs, one RDD per batch interval.

  <img src="./assets/images/002.png" />

</div>

  <div>
    <br/><br/>
    Next we use the <a href="https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html" target="_workshop">DStream</a> foreachRDD method to apply processing to each RDD in this DStream.
  </div>
<pre>
// for each RDD. performs function on each RDD in DStream
linesDStream.foreachRDD(rdd=>{
   val sensorDStream = rdd.map(_.split(",")).map(
      p => Sensor(
        p(0),
        p(1),
        p(2),
        p(3).toDouble,
        p(4).toDouble,
        p(5).toDouble,
        p(6).toDouble,
        p(7).toDouble,
        p(8).toDouble)
   )

   sensorDStream.take(2).foreach(println)
})
</pre>

    <div>
      The map operation applied on each entry to create a new Sensor object.
      <br/>
      <img src="./assets/images/003.png" />
    </div>



    <br/><br/>
    <h5>Start Receiving Data</h5>
    <div>
      To start receiving data, we must explicitly call start() on the StreamingContext, then call awaitTermination to wait for the streaming computation to finish.

    </div>
<pre>
// Start the computation
ssc.start()

// Wait for the computation to terminate
ssc.awaitTermination()
</pre>


    <br/><br/>
    <h4>Save data that will be captured by Spark</h4>
    <div>
      Using another terminal window login into your cluster:
      <pre>ssh [$userid]]@ipaddress</pre>
    </div>

    <div>
      <br/><br/>
      Copy the <code>sensordata.csv</code> file from the streaminglab/data directory to the stream directory (the directory that the streaming application will read from) at the linux command line type:
      <pre>cp  /user/$USER/data/sensordata.csv /user/$USER/stream/.</pre>
    </div>

    <div>
      <br/>
      The window with the shell should print out information about parsing the data.
    </div>


    <br/><br/>
    <h4>Observe Streaming Application in Web UI</h4>
    <div>
      Launch the Spark Streaming UI  in the browser with you sandbox or cluster ipaddress and port 4040: (or the port started by Spark UI)
      <ul>
        <li>http://ipaddress:4040</li>
        <li>Click on the Streaming tab. </li>
      </ul>
    </div>



    <br/><br/>
    <h4>Summary</h4>
    <div>
      In this section, you have learned how to use the Spark Streaming API. Using the Spark Shell the application capture each new file in the <code>[userid]/stream</code> folder and create a new RDD based on the Sensor class.
      <br/><br/>
      You have also learned how to use the Spark Web UI to observe your streaming application.
    </div>


</section>


<h1>Stream data in real time with Kafka/  MapR Streams</h1>
<section id="lab-04">

<p>
  So far the data are sent using simple file, and captured by Spark Streaming. Spark can also directly consume data/events coming from Kafka. Before integrating Kafka and Spark let's write a very simple Kafka application.
</p>

<p>

<ul>
<li>One or more <code>producers</code> send message to the cluster on a specific <code>topic</code></li>
<li>One or more <code>consumer</code> subscribe to the topic and consume the messages </li>
</ul>
</p>

<p>
  For this example, instead of a Kafka cluster, you will be using Mapr Streams, that use the Apache Kafka 0.9 API but has a different broker. Note that the code used for Kafka is identical, only the configurations and topic notation are different.
</p>

<p>
The first thing to do when you work with MapR Stream is to create a "stream" that will contains the various topic. In Apache Kafka you do not have the notion of Streams, you just create the topic on your broker. MapR has added the
notion of stream to allow administrator to better manage permissions, retention policy and advanced features such as cross datacenter replication.
</p>

<p>
  Let's create a new stream and a new topic for the application: (replace user01 by your user)

<pre>maprcli stream create -path /user/$USERID/pump -produceperm p -consumeperm p -topicperm p

maprcli stream topic create -path /user/$USERID/pump -topic sensor
</pre>

Here you create a named "pump" with public permissions (<code>-produceperm p -consumeperm p -topicperm p</code>);<br/>
and create a new topic "sensor" in the pump stream.

</p>


<h3>Produce and Consume messages using Apache Kafka tools</h3>

<p>
	You have now created the topic <code>pump</code>, before creating a Java application to send message and consume them with Spark, let's use Kafka tools to use the topic.
</p>

<p>
	Open 2 news terminals, one for the "producer" the other for the "consumer" tool, and run the following commands:
</p>

Producer:
<pre>
/opt/mapr/kafka/kafka-0.9.0/bin/kafka-console-producer.sh --topic /user/$USERID/pump:sensor --broker-list this.will.be.ignored:9092
</pre>

Consumer:
<pre>
/opt/mapr/kafka/kafka-0.9.0/bin/kafka-console-consumer.sh --topic /user/$USERID/pump:sensor --new-consumer --bootstrap-server this.will.be.ignored:9092
</pre>

<p>
	In the producer terminal window, paste the following sensor <i>(a line from the CSV file)</i>:
	<pre>BBKING,3/14/14,23:46,9.78,0.943,1088,0.24,88,1.86</pre>
</p>

<p>
You can post as many messages as you want, or pipe the content of a file into the producer tools.
<br/><br/>
Let's now create a Java application that produces messages and a Spark Streaming job that consumes the messages.
</p>




<h3>Create a producer</h3>

<p>
Open the <a href="https://github.com/tgrall/mapr-streams-spark-hbase-workshop/blob/master/src/main/java/solution/MyProducer.java" target="_new"><code>[project]/java/solution/MyProducer</code></a> Java class.
</p>

<p>
  This class is a simple Java application that:
  <ul>
    <li>Get the name "topic" name from the command line <code>args[0]</code></li>
    <li>Configure the "producer" in the <code>configureProducer()</code>, this is where you put all the properties needed by a producer, for example which serializers. You can find many configuration options for the producer (<a href="http://maprdocs.mapr.com/51/#MapR_Streams/configuration_parameters_for_producers.html" target="_new">MapR Streams</a> | <a href="http://kafka.apache.org/documentation.html#producerconfigs" target="_new">Kafka</a>) </li>
    <li>Then the application read the CSV file and send a message on the topic for each line</li>
  </ul>

When the <code>ProducerRecord</code> is created, it is created with topic, and a value, but also in this example a key that it is used to partition the publishing of the message on various topic partitions. (for scalability reason)

</p>

<h3>Create a Consumer</h3>

<p>
Open the <a href="https://github.com/tgrall/mapr-streams-spark-hbase-workshop/blob/master/src/main/java/solution/MyConsumer.java" target="_new"><code>[project]/java/solution/MyConsumer</code></a> Java class.
</p>

<p>
  This class is a simple Java application that:
  <ul>
    <li>Get the name "topic" name from the command line <code>args[0]</code></li>
    <li>Configure the "consumer" in the <code>configureConsumer()</code>, this is where you put all the properties needed by a producer, for example which serializers. You can find many configuration options for the consumer (<a href="http://maprdocs.mapr.com/51/#MapR_Streams/configuration_parameters_for_consumers.html" target="_new">MapR Streams</a> | <a href="http://kafka.apache.org/documentation.html#consumerconfigs" target="_new">Kafka</a>) </li>
    <li>Messages are received from the cluster and just printed.</li>
  </ul>

The consumer will read the messages from the last offset user by this consumer group.

</p>


<h2>Run the Producer/Consumer Applications</h2>

<p>
  On your laptop, in your IDE or from the command line , build the project using maven.
<pre>
  mvn clean package</pre>
</p>

<p>
Copy the jar to your MapR cluster:
<pre>
  scp  target/ms-sparkstreaming-1.0.jar user01@ipaddress:/user/$USER/.

  // if you are using virtualbox:
  scp -P 2222 ms-sparkstreaming-1.0.jar user01@127.0.0.1:/user/user01/.
</pre>
</p>


<h3>Run the application</h3>
<p>
Open two terminal windows, with the same user $USER, and go to:
<pre>
  ssh user01@ipaddress –p port
  cd /user/$USER</pre>
</p>

<p>
In the other window, run the consumer with the following command:

<pre>
java -cp ms-sparkstreaming-1.0.jar:`mapr classpath` solution.MyConsumer /user/$USER/pump:sensor
</pre>
</p>


<p>
In the one window, run the producer with the following command:

<pre>
java -cp ms-sparkstreaming-1.0.jar:`mapr classpath` solution.MyProducer /user/$USER/pump:sensor
</pre>
</p>

<p>
  As you can see the producer sends the messages to the topic and it is automtically capture by the consumer.<br/>

  Also, and this is important when you are building stream based application, the consumer will catchup with all the message it has not yet consumed, to test this:

<ul>
  <li>1- Kill the consumer</li>
  <li>2- Run the producer</li>
  <li>3- Start the consumer: you'll see that the messages are retrieved automatically from the cluster.</li>
</ul>
</p>


</section>

<h1>Consume Kafka messages into a Spark Streaming Application</h1>
<section id="lab-05">

<p>
You have now the basis of Spark Streaming, and Kafka. The idea now is to connect Spark as a "consumer", of the <code>pump:sensor</code> topic.<br/><br/>

Spark has interesting Maven dependencies, included in this project to integrate with Apache Kafka, so MapR Streams:

<pre>
  &lt;dependency&gt;
      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
      &lt;artifactId&gt;spark-streaming-kafka-0-9_2.11&lt;/artifactId&gt;
      &lt;version&gt;2.0.1-mapr-1611&lt;/version&gt;
  &lt;/dependency&gt;
</pre>
</p>

<h2>Create a Spark Application</h2>

  <p>
    The first part of the code is very similar to the code you have written in the previous lab, except that now you will write it in the context of an application: A Scala Class with a main function.  (This could also be a Java Class.)
  </p>


  <p>
    Open the <a href="https://github.com/tgrall/mapr-streams-spark-hbase-workshop/blob/master/src/main/scala/solution/SensorStreamConsumer.scala" target="_new"><code>[project]/src/main/scala/solution.SensorStreamConsumer</code></a> and look at the configuration.
  </p>

  <p>
    This is simple Scala application with a <code>main</code> method with the following logic:

    <ul>
      <li>Get the name "topic" name from the command line <code>args[0]</code></li>
      <li>Configure the Spark Kafka Consumer with the following properties (such as groupId, topic, serializers, ... )</li>
      <li>Then use the KafkaUtils.createDirectStream to create a Spark Streaming RDD</li>
    </ul>
  </p>

<p>
The application get the data as simple RDD, and then you can, like you have done with the shell convert the RDD into another one using the Sensor class.

<pre>
  val consumerStrategy = ConsumerStrategies.Subscribe[String, String](topicsSet, kafkaParams)
  val linesDStream = KafkaUtils.createDirectStream[String, String](
    ssc, LocationStrategies.PreferConsistent, consumerStrategy
  )

  val sensorDStream = linesDStream.map(_.value()).map(parseSensor)
</pre>

This looks as follow:
<div><img src="./assets/images/003.png"/></div>
</p>

<h3>Run the application</h3>

<p>
  Open a new terminal window on your cluster and run the following command:

<pre>
  /opt/mapr/spark/spark-2.0.1/bin/spark-submit \
    --class solution.SensorStreamConsumer \
    --master local[2] ms-sparkstreaming-1.0.jar /user/$USER/pump:sensor
</pre>
</p>


<p>
  You can see Spark taking the messages and doing the aggregations, done on the data received each 2 seconds (configured in the consumer), you have options to do additional operations for example windowed operations which allow you to apply transformations over a sliding window of data.
<p>



</section>

<h1>Build and Run a Streaming Application which Writes to HBase</h1>
<section id="lab-005">

  <p>
Let's create a new version of the application and save the data into HBase.
  </p>

  <div>
    <h5>HBase Table schema</h5>

    The HBase Table Schema for the streaming data is as follows:
    <ul>
      <li>Composite row key of the pump name date and time stamp</li>
      <li>Column Family data with columns corresponding to the input data fields</li>
      <li>Column Family alerts with columns  corresponding to any filters for alarming values</li>
      <li>Note that the data and alert column families could be set to expire values after a certain amount of time.</li>
    </ul>
    The Schema for the daily statistics summary rollups is as follows:
    <ul>
      <li>Composite row key of the pump name and date</li>
      <li>Column Family stats</li>
      <li>Columns for min, max, avg.</li>
    </ul>

    <img src="./assets/images/005.png" />
  </div>


  <div>
    <br/>
    <h5>Saving Sensor data into HBase</h5>

    The function below converts a Sensor object into an HBase Put object, which is used to insert a row into HBase.
<pre>
val cfDataBytes = Bytes.toBytes("data")
object Sensor {

. . .

//  Convert a row of sensor object data to an HBase put object
def convertToPut(sensor: Sensor): (ImmutableBytesWritable, Put) = {
  val dateTime = sensor.date + " " + sensor.time

  // create a composite row key: sensorid_date time
  val rowkey = sensor.resid + "_" + dateTime
  val put = new Put(Bytes.toBytes(rowkey))

  // add to column family data, column  data values to put object
  put.add(cfDataBytes, Bytes.toBytes("hz"), Bytes.toBytes(sensor.hz))
  put.add(cfDataBytes, Bytes.toBytes("disp"), Bytes.toBytes(sensor.disp))
  put.add(cfDataBytes, Bytes.toBytes("flo"), Bytes.toBytes(sensor.flo))
  put.add(cfDataBytes, Bytes.toBytes("sedPPM"), Bytes.toBytes(sensor.sedPPM))
  put.add(cfDataBytes, Bytes.toBytes("psi"), Bytes.toBytes(sensor.psi))
  put.add(cfDataBytes, Bytes.toBytes("chlPPM"), Bytes.toBytes(sensor.chlPPM))
  return (new ImmutableBytesWritable(Bytes.toBytes(rowkey)), put)
  }
}
</pre>

  </div>


  <div>
    <br/>
    <h5>Saving PSI Alert data into HBase</h5>

    The function use the PSI alert, and save it into another column family ("alert"), and column ("psi").
<pre>
  def convertToPutAlert(sensor: Sensor): (ImmutableBytesWritable, Put) = {
    val dateTime = sensor.date + " " + sensor.time
    // create a composite row key: sensorid_date time
    val key = sensor.resid + "_" + dateTime
    val p = new Put(Bytes.toBytes(key))
    // add to column family alert, column psi data value to put object
    p.addColumn(cfAlertBytes, colPsiBytes, Bytes.toBytes(sensor.psi))
    return (new ImmutableBytesWritable(Bytes.toBytes(key)), p)
  }

</pre>

  </div>


  <div>
The Spark streaming application logic is more or less the same as before:
<ul>
  <li>Get the name "topic" name from the command line <code>args[0]</code></li>
  <li>Configure the Spark Kafka Consumer with the following properties (such as groupId, topic, serializers, ... )</li>
  <li>Then use the KafkaUtils.createDirectStream to create a Spark Streaming RDD</li>
  <li>Save the RDD entries into HBase using the method defined above</li>
  <li>Use a filter to find alerts on PSI on add an attribute to the column family "alerts"</li>
</ul>
  </div>


  <h3>Create the HBase Table</h3>

  <p>
Launch the HBase Shell and create the table

  <pre>
mkdir /user/$USER/db/

hbase shell

# Replace $USER by your user

create '/user/$USER/db/sensor', {NAME=>'data'}, {NAME=>'alert'}, {NAME=>'stats'}
  </pre>
  </p>



  <h3>Run the application</h3>

  <p>
    Open a new terminal window on your cluster and run the following commands:

  <pre>
/opt/mapr/spark/spark-2.0.1/bin/spark-submit \
  --class solution.HBaseSensorStreamConsumer \
  --master local[2] \
  ms-sparkstreaming-1.0.jar /user/$USER/pump:sensor /user/$USER/db/sensor
  </pre>
  </p>

  <p>
    When you run this program you will see the spark application reading the sensor data and save data into the database
  </p>

  <p>
    You can now scan the table using the hbase shell

<pre>
hbase shell

# Replace $USER by your user

get '/user/$USER/db/sensor', 'THERMALITO_3/14/14 9:59'

get '/user/$USER/db/sensor', 'NANTAHALLA_3/13/14 2:05'

scan '/user/$USER/db/sensor'
</pre>
  </p>
</section>



<br/><br/>
<h1>Conclusion</h1>
<section id="lab-06">

  <br/>
  <div>
    In this workshop you have learned how to:
    <ul>
      <li>use Apache Spark to digest CSV data</li>
      <li>use DataFrame to do advanced queries on these data</li>
      <li>use Spark Streaming to stream files into the system automatically</li>
      <li>write and execute a Kafka Producer and a Consumer</li>
      <li>create a Spark application that read Kafka topic to get event in real time</li>
      <li>save data parsed using Spark into Apache Hbase</li>
    </ul>

    <p>
      This workshop is a simple introduction to Time Series application showing how to capture "sensor data", in this case coming from files, then save and process these data. If you want to learn more about Time Series you can look at the <a href="https://www.mapr.com/time-series-databases-new-ways-store-and-access-data" target="_workshop">ebook</a>
    </p>


    <p>
      If you want learn more about Apache Spark, HBase, register for the free online training at <a href="http://learn.mapr.com" target="_workshop">http://learn.mapr.com</a>, read the <a href="https://www.mapr.com/getting-started-apache-spark" target="_workshop">online Spark eBook</a>
    </p>

  <div/>

</section>


</div>




    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="assets/js//jquery.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="assets/js/bootstrap.min.js"></script>
  </body>
</html>
